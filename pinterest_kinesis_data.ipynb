{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c6cf32-638f-4741-9001-76a65baf58e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/Workspace/Users/holmes_jennifer@ymail.com/pinterest-data-pipeline\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from functools import reduce\n",
    "\n",
    "from pinterest_batch_data import clean_df_pin, clean_df_geo, clean_df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a41ee75-2a99-4bea-a384-6e69a0628438",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6276a4-8cf1-4ccd-92f7-4aa36674ca24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f27283e-dc24-48cd-a679-17114ee9a405",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb432b7-4aff-485f-bda2-b47b71231745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the data stream\n",
    "pin_struct = StructType([\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"downloaded\", IntegerType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read data from Kinesis stream\n",
    "df_pin = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-0afff69adbe3-pin') \\\n",
    "    .option('initialPosition', 'latest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the binary data to JSON string\n",
    "df_pin = df_pin.selectExpr(\"CAST(data AS STRING) jsonData\")\n",
    "\n",
    "# Parse the JSON string into a DataFrame with the defined schema\n",
    "df_pin = df_pin.select(from_json(\"jsonData\", pin_struct).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Clean the DataFrame using a custom cleaning function\n",
    "df_pin = clean_df_pin(df_pin)\n",
    "\n",
    "# Display the DataFrame for visualization\n",
    "display(df_pin)\n",
    "\n",
    "# Write the stream data to a Delta table with checkpointing\n",
    "df_pin.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "    .table(\"0afff69adbe3_pin_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24d80bfe-212c-4920-873c-de20959d12b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the data stream\n",
    "geo_struct = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read data from the Kinesis stream\n",
    "df_geo = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-0afff69adbe3-geo') \\\n",
    "    .option('initialPosition', 'latest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "# Convert the binary data to JSON string\n",
    "df_geo = df_geo.selectExpr(\"CAST(data AS STRING) jsonData\")\n",
    "\n",
    "# Parse the JSON string into a DataFrame with the defined schema\n",
    "df_geo = df_geo.select(from_json(\"jsonData\", geo_struct).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Clean the DataFrame using a custom cleaning function\n",
    "df_geo = clean_df_geo(df_geo)\n",
    "\n",
    "# Display the DataFrame for visualization\n",
    "display(df_geo)\n",
    "\n",
    "# Write the stream data to a Delta table with checkpointing\n",
    "df_geo.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "    .table(\"0afff69adbe3_geo_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f7ee2e7-313c-4d38-8723-a488240e6438",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the data stream\n",
    "user_struct = StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"last_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read data from the Kinesis data stream\n",
    "df_user = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-0afff69adbe3-user') \\\n",
    "    .option('initialPosition', 'latest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "# Convert the binary data to JSON string\n",
    "df_user = df_user.selectExpr(\"CAST(data AS STRING) jsonData\")\n",
    "\n",
    "# Parse the JSON string into a DataFrame with the defined schema\n",
    "df_user = df_user.select(from_json(\"jsonData\", user_struct).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Clean the DataFrame using a custom cleaning function\n",
    "df_user = clean_df_user(df_user)\n",
    "\n",
    "# Display the DataFrame for visualization\n",
    "display(df_user)\n",
    "\n",
    "# Write the stream data to a Delta table with checkpointing\n",
    "df_user.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "    .table(\"0afff69adbe3_user_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 764650334901796,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pinterest_kinesis_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
